{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python optimisation variables\n",
    "learning_rate = 0.01\n",
    "epochs = 2000\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "newsgroups_train = fetch_20newsgroups(subset=\"train\")\n",
    "from sklearn.feature_extraction import stop_words\n",
    "newsgroups_trainlabel = newsgroups_train.target\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words.ENGLISH_STOP_WORDS)\n",
    "vectorstrain = vectorizer.fit_transform(newsgroups_train.data)\n",
    "newsgroups_test = fetch_20newsgroups(subset=\"test\")\n",
    "vocab = vectorizer.vocabulary_\n",
    "newvectorizer = TfidfVectorizer(vocabulary=vocab)\n",
    "vectorstest = newvectorizer.fit_transform(newsgroups_test.data)\n",
    "newsgroups_testlabel = newsgroups_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 129796)\n",
      "(7532, 1000)\n",
      "(11314, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "print(vectorstrain.shape)\n",
    "X_new = SelectKBest(chi2, k=1000).fit(vectorstrain, newsgroups_trainlabel)\n",
    "chi2result = np.where(X_new.get_support())[0]\n",
    "vectorstestchi2 = vectorstest[:,chi2result]\n",
    "print(vectorstestchi2.shape)\n",
    "vectorstrainchi2 = vectorstrain[:,chi2result]\n",
    "print(vectorstrainchi2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 20)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(7532, 20)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "hot_vector_label_train = np.zeros((newsgroups_trainlabel.shape[0],20))\n",
    "print(hot_vector_label_train.shape)\n",
    "index = 0\n",
    "for label in newsgroups_trainlabel:\n",
    "    #print(label)\n",
    "    hot_vector_label_train[index][int(label)] = 1\n",
    "    index += 1\n",
    "\n",
    "print(hot_vector_label_train)\n",
    "\n",
    "index = 0\n",
    "\n",
    "hot_vector_label_test = np.zeros((newsgroups_testlabel.shape[0],20))\n",
    "print(hot_vector_label_test.shape)\n",
    "index = 0\n",
    "for label in newsgroups_testlabel:\n",
    "    #print(label)\n",
    "    hot_vector_label_test[index][int(label)] = 1\n",
    "    index += 1\n",
    "\n",
    "print(hot_vector_label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 129796)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomn_selection(X_train,label_train,k):\n",
    "    idx = np.random.randint(X_train.shape[0], size=k)\n",
    "    return X_train[idx],label_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = tf.placeholder(tf.float32, [None, vectorstrainchi2.shape[1]], name='InputData')\n",
    "# Documents => 20 classes\n",
    "y = tf.placeholder(tf.float32, [None, 20], name='LabelData')\n",
    "\n",
    "# now declare the weights connecting the input to the hidden layer\n",
    "print(vectorstrainchi2.shape[1])\n",
    "W1 = tf.Variable(tf.random_normal([vectorstrainchi2.shape[1], 32],stddev=0.01), name='W1')\n",
    "b1 = tf.Variable(tf.random_normal([32]), name='b1')\n",
    "\n",
    "# and the weights connecting the hidden layer to the output layer\n",
    "W2 = tf.Variable(tf.random_normal([32, 20],stddev=0.1), name='W3')\n",
    "b2 = tf.Variable(tf.random_normal([20]), name='b3')\n",
    "\n",
    "hidden_out = tf.add(tf.matmul(x, W1), b1)\n",
    "hidden_out = tf.nn.relu(hidden_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Model'):\n",
    "    # Model\n",
    "    pred = tf.nn.softmax(tf.add(tf.matmul(hidden_out, W2), b2))\n",
    "with tf.name_scope('Loss'):\n",
    "    # Minimize error using cross entropy\n",
    "    cost = -tf.reduce_mean(tf.reduce_sum(y * tf.log(pred)+ (1 - y) * tf.log(1 - pred), axis=1))\n",
    "with tf.name_scope('SGD'):\n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "with tf.name_scope('Accuracy'):\n",
    "    # define an accuracy assessment operation\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(pred, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally setup the initialisation operator\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 1000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstrainchi2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "Step 0: Minibatch Loss: 4.249251\n",
      "0.041157726\n",
      "Step 100: Minibatch Loss: 3.950016\n",
      "0.098114714\n",
      "Step 200: Minibatch Loss: 3.937948\n",
      "0.0893521\n",
      "Step 300: Minibatch Loss: 3.927655\n",
      "0.17817312\n",
      "Step 400: Minibatch Loss: 3.892547\n",
      "0.2940786\n",
      "Step 500: Minibatch Loss: 3.854112\n",
      "0.38568774\n",
      "Step 600: Minibatch Loss: 3.746082\n",
      "0.42485395\n",
      "Step 700: Minibatch Loss: 3.642979\n",
      "0.44941583\n",
      "Step 800: Minibatch Loss: 3.433110\n",
      "0.4731811\n",
      "Step 900: Minibatch Loss: 3.117323\n",
      "0.5207116\n",
      "Step 1000: Minibatch Loss: 2.871674\n",
      "0.57116306\n",
      "Step 1100: Minibatch Loss: 2.782899\n",
      "0.59121084\n",
      "Step 1200: Minibatch Loss: 2.563456\n",
      "0.62360597\n",
      "Step 1300: Minibatch Loss: 2.206447\n",
      "0.6419278\n",
      "Step 1400: Minibatch Loss: 2.177583\n",
      "0.65932024\n",
      "Step 1500: Minibatch Loss: 1.911677\n",
      "0.6735263\n",
      "Step 1600: Minibatch Loss: 1.874744\n",
      "0.68414766\n",
      "Step 1700: Minibatch Loss: 1.749049\n",
      "0.6950345\n",
      "Step 1800: Minibatch Loss: 1.624669\n",
      "0.7047265\n",
      "Step 1900: Minibatch Loss: 1.418243\n",
      "0.7100372\n",
      "Step 2000: Minibatch Loss: 1.364629\n",
      "0.7126925\n",
      "Step 2100: Minibatch Loss: 1.509844\n",
      "0.71641\n",
      "Step 2200: Minibatch Loss: nan\n",
      "0.042352628\n",
      "Step 2300: Minibatch Loss: nan\n",
      "0.042352628\n",
      "Step 2400: Minibatch Loss: nan\n",
      "0.042352628\n",
      "Step 2500: Minibatch Loss: nan\n",
      "0.042352628\n",
      "Step 2600: Minibatch Loss: nan\n",
      "0.042352628\n",
      "Step 2700: Minibatch Loss: nan\n",
      "0.042352628\n",
      "Step 2800: Minibatch Loss: nan\n",
      "0.042352628\n",
      "Step 2900: Minibatch Loss: nan\n",
      "0.042352628\n",
      "Step 3000: Minibatch Loss: nan\n",
      "0.042352628\n",
      "Step 3100: Minibatch Loss: nan\n",
      "0.042352628\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-3be2f9912621>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mavg_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandomn_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorstrainchi2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhot_vector_label_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_x\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "total_batch = int(vectorstrainchi2.todense().shape[0] / batch_size)\n",
    "print(total_batch)\n",
    "for epoch in range(epochs):\n",
    "    avg_cost = 0\n",
    "    for i in range(total_batch):\n",
    "        batch_x, batch_y = randomn_selection(vectorstrainchi2.todense(), hot_vector_label_train,batch_size)\n",
    "        l, c = sess.run([optimizer, cost], feed_dict={x:batch_x , y:batch_y})   \n",
    "    if(epoch % 100 == 0):\n",
    "        print('Step %i: Minibatch Loss: %f' % (epoch, c))\n",
    "        print(sess.run(accuracy, feed_dict={x: vectorstestchi2.todense(), y: hot_vector_label_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
